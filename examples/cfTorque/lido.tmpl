## Job Resource Interface Definition
##
## nodes [integer(1)]:        Number of required nodes, 
##                            Set larger than 1 if you want to further parallelize with e.g. MPI
## walltime [integer(1)]:     Walltime for this job, in seconds
## memory   [integer(1)]:     Memory in megabytes for the job. Mapped to vmem on Lido.
##                            If you use multiple nodes, vmem = memory * nodes.
##                            Therefore memory is the max mem you want to use in one parallel (MPI) job.
## compiler [gcc,icc]:        Compiler for R and packages
##                            We have two different R versions on LiDO installed, one with GCC
##                            and the other with the Intel compilers. Some packages fail to build
##                            with ICC, but R compiled with ICC is in general faster.
##
## Default resources can be set in your BatchJobs.conf by defining the variable 
## 'default.resources' as a named list.

<%
d = setdiff(names(resources), c("walltime", "memory", "nodes", "compiler"))
if (length(d) > 0)
  stopf("Illegal resources used: %s", collapse(d))

walltime = convertInteger(resources$walltime)
memory = convertInteger(resources$memory)
nodes = convertInteger(resources$nodes)
compiler = resources$compiler
    
checkArg(walltime, "integer", len=1L, na.ok=FALSE, lower=60, upper=60*60*24*28)
checkArg(memory, "integer", len=1L, na.ok=FALSE, lower=1, upper=64*1024)
checkArg(nodes, "integer", len=1L, na.ok=FALSE, lower=1)
checkArg(compiler, choices=c("gcc", "icc"))
memory = memory * nodes
if (memory > 64*1024)
  stop("memory * nodes exceeds 64GB!")

cmd = "R CMD BATCH --no-save --no-restore"
if(nodes > 1L) 
    cmd = paste("mpirun -np 1", cmd)

## choose a suitable queue for job
s2 = ifelse(memory <= 16*1024, "eth", "quad")
day = 3600 * 24
s1 = if (walltime <= 3600) {
  "short"
} else if (walltime <= 8 * 3600) {
  "med"
} else if (walltime <= 2 * day) {
  "long"
} else if (walltime <= 28 * day) {
  "ultralong"
}
if (s1 == "ultralong" && s2 == "quad")  
  stop("No 'ultralong_quad' queue available. Reduce memory or walltime.")
queue = paste(s1, s2, sep="_")

## get the right software modules to load depending on
## parallelization (nodes > 1) and compiler
modules = switch(compiler,
  "icc" = c("openmpi/ge/intel11.1/64/1.4.2", "intel/cce/11.1.075", "intel/fce/11.1.075", "R/2.14.1-icc"),
  "gcc" = c("openmpi/ge/gcc4.3.5/64/1.4.2", "gcc/4.3.5", "R/2.15.1-gcc"))
if(nodes == 1L)
  modules = modules[-1]
modules = collapse(modules, sep=" ")

## very ugly hack because we cannot log to data (nobackup) filesystem on lido,
## only home fs is available 
## unfortunately there seems to be no generic solution
## does log path start with /data/?
if (length(grep("^/data/", log.file)) > 0) {
  ## strip that
  log.file2 = substr(log.file, 7, nchar(log.file))
  ## find next forward slash
  i = regexpr("/", log.file2)
  if (i != -1) {
    ## this must be "user": e.g. /data/bischl/...
    user = substr(log.file2, 1, i-1)
    ## put together
    log.file = sprintf("/home/%s/nobackup%s", user, substr(log.file2, i, nchar(log.file2)))
  }
}
-%>

#PBS -N <%= job.name %>
#PBS -j oe
#PBS -o <%= log.file %>
#PBS -l walltime=<%= walltime %>,nodes=<%= nodes %>,vmem=<%= memory %>M
#PBS -q <%= queue %>

## setup modules
source /sysdata/shared/sfw/Modules/default/init/bash
module add <%= modules %>

## create our own temp dir and clean it up later, lido does not do this automatically 
mkdir /scratch/${USER}-${PBS_JOBID}
export TMPDIR=/scratch/${USER}-${PBS_JOBID}
## run R
<%= cmd %> "<%= rscript %>" /dev/stdout
rm -fR /scratch/${USER}-${PBS_JOBID}
